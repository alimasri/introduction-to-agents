{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to AI Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ali Masri\n",
    "### Machine Learning Engineer @ Ford Motor Company\n",
    "\n",
    "### Let us Connect\n",
    "\n",
    "- [www.linkedin.com/in/alimasri](https://www.linkedin.com/in/alimasri)\n",
    "- [github.com/alimasri](http://github.com/alimasri)\n",
    "- [www.alimasri.info](https://www.alimasri.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Agents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Generative AI agent can be defined as an application that attempts to achieve a **goal** by **observing** the world and **acting upon it** using the **tools** that it has at its disposal. Agents are **autonomous** and can act **independently** of human intervention - [**Google Agents White Paper**](https://www.kaggle.com/whitepaper-agents)\n",
    "\n",
    "![Agent Runtime](./assets/agentruntime.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keys to Building Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Keys to Building Agents](./assets/agents.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Low-level Orchestration\n",
    "  - [LangGraph](https://langchain-ai.github.io/langgraph/)\n",
    "- High-level\n",
    "  - [AutoGen](https://www.microsoft.com/en-us/research/project/autogen/)\n",
    "  - [PydanticAI](https://ai.pydantic.dev)\n",
    "  - [CrewAI](https://www.crewai.com/)\n",
    "  - [Agent Development Kit (ADK)](https://google.github.io/adk-docs/)\n",
    "- Code Generation\n",
    "  - [SmolAgents](https://github.com/huggingface/smolagents)\n",
    "- No-Code\n",
    "  - [n8n](https://n8n.io/)\n",
    "  - [CopilotStudio](https://www.microsoft.com/en-us/microsoft-copilot/microsoft-copilot-studio)\n",
    "  - [Rivet](https://rivet.ironcladapp.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install uv <https://docs.astral.sh/uv/getting-started/installation/>\n",
    "   1. Mac and Linux:\n",
    "      ```bash\n",
    "      curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "      ```\n",
    "   2. Windows:\n",
    "      ```bash\n",
    "      powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n",
    "      ```\n",
    "2. Install depencenies:\n",
    "    ```bash\n",
    "    uv sync\n",
    "    ```\n",
    "3. Setup environment variables:\n",
    "   1. Make a copy of `.env.example` and rename it to `.env_vars`\n",
    "      ```bash\n",
    "      cp .env.example .env_vars\n",
    "      ```\n",
    "   2. Fill in the values for the environment variables in the `.env_vars` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env_vars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting to an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "DEFAULT_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "llm = ChatOpenAI(model=DEFAULT_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"Write one sentence about the Sun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(PROMPT).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Role Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLE = \"You are a 5 years old kid.\"\n",
    "llm.invoke(f\"{ROLE} {PROMPT}\").pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\n",
    "    input=[\n",
    "        (\"system\", ROLE),\n",
    "        (\"human\", PROMPT),\n",
    "    ]\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Knowledge Cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# Real-time Information - FAIL\n",
    "Markdown(llm.invoke(\"What are the latest AI models from OpenAI\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 129012291\n",
    "b = 12912\n",
    "math_problem = f\"What is the value of {a} x {b}?\"\n",
    "print(math_problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(llm.invoke(math_problem).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{a * b:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<https://python.langchain.com/docs/concepts/tool_calling/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tools Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers\n",
    "\n",
    "    Args:\n",
    "        a: The first number\n",
    "        b: The second number\n",
    "\n",
    "    Returns:\n",
    "        int: The product of the two numbers\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get the weather of a city\n",
    "\n",
    "    Args:\n",
    "        city (str): The city to get the weather of\n",
    "\n",
    "    Returns:\n",
    "        str: The weather of the city\n",
    "    \"\"\"\n",
    "    import requests\n",
    "\n",
    "    return requests.get(f\"https://wttr.in/{city}\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tool Binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tool = llm.bind_tools(tools=[multiply, get_weather])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tool Calling Internals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [math_problem]\n",
    "\n",
    "ai_response = llm_with_tool.invoke(math_problem)\n",
    "messages.append(ai_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_response.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tool Calling](./assets/tool_calling.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_response = multiply.invoke(ai_response.tool_calls[0])\n",
    "messages.append(tool_response)\n",
    "tool_response.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_response = llm_with_tool.invoke(messages)\n",
    "messages.append(ai_response)\n",
    "ai_response.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Tool Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_response = llm_with_tool.invoke(\"What is the weather in Beirut? Multiply that by 3\")\n",
    "ai_response.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\"What is the weather in Beirut? Multiply that by 3\"]\n",
    "tools = {\"multiply\": multiply, \"get_weather\": get_weather}\n",
    "while True:\n",
    "    ai_response = llm_with_tool.invoke(messages)\n",
    "    ai_response.pretty_print()\n",
    "    messages.append(ai_response)\n",
    "    if not ai_response.tool_calls:\n",
    "        break\n",
    "    for tool_call in ai_response.tool_calls:\n",
    "        tool_response = tools[tool_call[\"name\"]].invoke(tool_call)\n",
    "        messages.append(tool_response)\n",
    "        tool_response.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic Design Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Agentic Design Patterns](assets/multi-agent-architectures.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReAct Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ReAct Agent](assets/react.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://langchain-ai.github.io/langgraph/prebuilt/#available-libraries\n",
    "from langgraph.prebuilt import create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "react_agent = create_react_agent(DEFAULT_MODEL, tools=[multiply, get_weather])\n",
    "react_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_response = react_agent.invoke({\"messages\": [\"What is the value of 129012291 x 12912\"]})\n",
    "for message in agent_response[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The agent can handle multiple tool calls\n",
    "response = react_agent.invoke(\n",
    "    {\"messages\": [\"Get today's weather in Beirut and multiply that by 3\"]}\n",
    ")\n",
    "for _ in response[\"messages\"]:\n",
    "    _.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = react_agent.invoke(\n",
    "    {\"messages\": [\"I live in Beirut, is today a good day for swimming in my pool?\"]}\n",
    ")\n",
    "for _ in response[\"messages\"]:\n",
    "    _.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sales Agent Demo\n",
    "\n",
    "```bash\n",
    "uv run code/sales.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CodeAgents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Motivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = \"rr1rrr2rrrr32rrrr121212rrrr1221\"\n",
    "rs.count(\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(f\"How many 'r's are there in `{rs}`?\").pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://platform.openai.com/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\n",
    "    f\"Write a python code to count the number of 'r's in `{rs}`? Don't answer the question, just write the code.\"\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/smolagents\n",
    "from smolagents import CodeAgent, OpenAIServerModel\n",
    "\n",
    "model = OpenAIServerModel(model_id=\"gpt-4o-mini\")\n",
    "codeagent = CodeAgent(model=model, tools=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codeagent.run(f\"How many 'r's are there in `{rs}`?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CodeAgents](assets/codeagent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real World Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\\\n",
    "Hello there, my name is John Doe. I live in Dearborn, Michigan. My phone number is 313-555-1212.\n",
    "\"\"\"\n",
    "\n",
    "problem = f\"\"\"\\\n",
    "Identify all PII information in the given text and return a list of:  \n",
    "- The PII category.  \n",
    "- The PII value.  \n",
    "- The starting index of the PII information in the string.  \n",
    "- The ending index of the PII information in the string.\n",
    "\n",
    "Note that indexes are 0-based.\n",
    "\n",
    "Text:\n",
    "{text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(ChatOpenAI(model=DEFAULT_MODEL).invoke(problem).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[18:27], text[34:53], text[61:76]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codeagent.run(problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[24:32], text[44:62], text[83:95]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CodeAgents with Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import CodeAgent, DuckDuckGoSearchTool\n",
    "\n",
    "search_tool = DuckDuckGoSearchTool()\n",
    "agent = CodeAgent(model=model, tools=[search_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = (\n",
    "    \"What is the USD/EUR exchange rate is the last 5 days? Report the min, max, and average values\"\n",
    ")\n",
    "agent.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervisor Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Supervisor](assets/langgraph-supervisor.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor = ChatOpenAI(model=DEFAULT_MODEL)\n",
    "\n",
    "instructions = \"\"\"\\\n",
    "You are a supervisor for a team of agents.\n",
    "Given the following context, you need to assign the question to the right agent.\n",
    "\n",
    "Available agents:\n",
    "\n",
    "- `chef`: A chef that can answer cooking questions\n",
    "- `doctor`: A doctor that can answer medical questions\n",
    "- `programmer`: An engineer that can answer technical questions\n",
    "\n",
    "If the user question is answered already by the agent, you can say `END`.\n",
    "\n",
    "<Context>\n",
    "{context}\n",
    "</Context>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor.invoke(\n",
    "    input=instructions.format(context=\"What are the main causes of migraine?\")\n",
    ").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class ResponseFormat(BaseModel):\n",
    "    agent_name: Literal[\"chef\", \"doctor\", \"programmer\", \"END\"] = Field(\n",
    "        ..., title=\"The name of the agent that should handle the message\"\n",
    "    )\n",
    "\n",
    "\n",
    "supervisor = llm.with_structured_output(ResponseFormat, method=\"function_calling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor.invoke(input=instructions.format(context=\"What are the main causes of migraine?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use the LangGraph Supervisor implementation <https://github.com/langchain-ai/langgraph-supervisor-py>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "chef = create_react_agent(\n",
    "    name=\"chef\",\n",
    "    model=ChatOpenAI(model=DEFAULT_MODEL),\n",
    "    tools=[],\n",
    "    prompt=\"You are a Michelin-starred chef that can provide recipes and cooking tips. Check the user's question and answer it accordingly.\",\n",
    ")\n",
    "\n",
    "doctor = create_react_agent(\n",
    "    name=\"doctor\",\n",
    "    model=ChatOpenAI(model=DEFAULT_MODEL),\n",
    "    tools=[],\n",
    "    prompt=\"You are a licensed physician that can provide medical advice. Check the user's question and answer it accordingly.\",\n",
    ")\n",
    "\n",
    "programmer = create_react_agent(\n",
    "    name=\"programmer\",\n",
    "    model=ChatOpenAI(model=DEFAULT_MODEL),\n",
    "    tools=[],\n",
    "    prompt=\"You are a senior software engineer that can help with coding problems. Check the user's question and answer it accordingly.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_supervisor import create_supervisor\n",
    "\n",
    "supervisor_builder = create_supervisor(\n",
    "    prompt=\"\"\"\\\n",
    "You are a supervisor managing a team of agents.  \n",
    "\n",
    "Your task is answer the user's question, by planning the right sequence of questions to ask the agents.\n",
    "  \n",
    "- **Do not attempt to answer the question yourself**—your role is strictly to delegate.  \n",
    "- **Ensure you only assign questions within an agent's area of expertise.** Avoid giving tasks they are not qualified for.  \n",
    "- **Review all agent responses** and compile the final answer into a clear and accurate report.\n",
    "\"\"\",\n",
    "    agents=[chef, doctor, programmer],\n",
    "    model=ChatOpenAI(model=DEFAULT_MODEL),\n",
    ")\n",
    "\n",
    "supervisor = supervisor_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"What are the main causes of migraine?\"\n",
    "response = supervisor.invoke({\"messages\": [message]})\n",
    "\n",
    "for _ in response[\"messages\"]:\n",
    "    _.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"\"\"\\\n",
    "Is it safe for a diabetic to eat caramelized chicken wings? Check the ingredients first, and then answer the question.\n",
    "\"\"\"\n",
    "response = supervisor.invoke({\"messages\": [message]})\n",
    "\n",
    "for _ in response[\"messages\"]:\n",
    "    _.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swarm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Swarm](assets/langgraph-swarm.png)\n",
    "\n",
    "<https://github.com/langchain-ai/langgraph-swarm-py>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan and Execute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Plan and Execute](assets/plan-and-execute.png)\n",
    "\n",
    "<https://langchain-ai.github.io/langgraph/tutorials/plan-and-execute/plan-and-execute/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoning Without Observation (ReWOO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ReWOO](assets/rewoo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Plan: I need to know the teams playing in the superbowl this year\n",
    "E1: Search[Who is competing in the superbowl?]\n",
    "Plan: I need to know the quarterbacks for each team\n",
    "E2: LLM[Quarterback for the first team of #E1]\n",
    "Plan: I need to know the quarterbacks for each team\n",
    "E3: LLM[Quarter back for the second team of #E1]\n",
    "Plan: I need to look up stats for the first quarterback\n",
    "E4: Search[Stats for #E2]\n",
    "Plan: I need to look up stats for the second quarterback\n",
    "E5: Search[Stats for #E3]\n",
    "```\n",
    "<https://langchain-ai.github.io/langgraph/tutorials/rewoo/rewoo/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LLM tracing is the practice of tracking and understanding the **step-by-step** decision-making and thought processes within LLMs as they generate responses.\n",
    "- Tracing helps to **debug** and **improve** the performance of LLMs.\n",
    "- **Available tools**\n",
    "  - [Langsmith](https://www.langchain.com/langsmith)\n",
    "  - [Arize Phoenix](https://phoenix.arize.com/)\n",
    "  - [Langfuse](https://langfuse.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Langsmith](assets/langsmith.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
